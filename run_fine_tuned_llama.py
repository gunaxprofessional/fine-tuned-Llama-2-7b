# -*- coding: utf-8 -*-
"""run fine tuned llama.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V70T2zk3WQRE_s9GV74BOXmX6x-y4n2T
"""

!pip -q install langchain huggingface_hub transformers sentence_transformers accelerate bitsandbytes

from langchain.llms import HuggingFacePipeline
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM,BitsAndBytesConfig

model_id = "Guna0pro/llama-2-7b-html"
tokenizer = AutoTokenizer.from_pretrained(model_id)



# Quantization configuration
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# Load base moodel
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map={"": 0}
)


pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=100
)

local_llm = HuggingFacePipeline(pipeline=pipeline)

from langchain import PromptTemplate, HuggingFaceHub, LLMChain

template = """You are a Great Web Developer, you have to help your junior regarding a question

              Question: {question}

              Answer: """

prompt = PromptTemplate(template=template, input_variables=["instruction"])

llm_chain = LLMChain(prompt=prompt,
                     llm=local_llm
                     )

question = "write a html code to create a basic form"

print(llm_chain.run(question))

