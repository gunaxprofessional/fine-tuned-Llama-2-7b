# -*- coding: utf-8 -*-
"""dataset creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NJWBi-09oTGb1C1mmgUVxvaRc-51krl1

**Good data is the foundation for creating a better model. It's like having a solid base for building something great.**
"""

# Install libraries
!pip install -q datasets transformers sentence_transformers faiss-gpu

from google.colab import userdata

# Defined in the secrets tab in Google Colab
hf_token = userdata.get('huggingface')

from datasets import load_dataset

# Load the dataset
dataset = load_dataset("ttbui/html_alpaca")
dataset

# Read as pandas DataFrame
dataset['train'].to_pandas()

from transformers import AutoTokenizer
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("NousResearch/Llama-2-7b-hf")

# 2. Tokenize each row and count the number of tokens
instruction_token_counts = [len(tokenizer.tokenize(example["instruction"])) for example in dataset['train']]
output_token_counts = [len(tokenizer.tokenize(example["output"])) for example in dataset['train']]
combined_token_counts = [instruction + output for instruction, output in zip(instruction_token_counts, output_token_counts)]

# Helper function to plot the distributions
def plot_distribution(token_counts, title):
    sns.set_style("whitegrid")
    plt.figure(figsize=(15, 6))
    plt.hist(token_counts, bins=50, color='#3498db', edgecolor='black')
    plt.title(title, fontsize=16)
    plt.xlabel("Number of tokens", fontsize=14)
    plt.ylabel("Number of examples", fontsize=14)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.show()

# Plot the distribution of token counts
plot_distribution(instruction_token_counts, "Distribution of token counts for instruction only")
plot_distribution(output_token_counts, "Distribution of token counts for output only")
plot_distribution(combined_token_counts, "Distribution of token counts for combined instruction + output")

"""## Near-deduplication using embeddings

How to choose the embedding model? Check the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard). In this example, we're not using the best embedding model because it would take too long.

![](https://i.imgur.com/Fbx1Ivz.png)
"""

from sentence_transformers import SentenceTransformer
import faiss
from datasets import Dataset, DatasetDict
from tqdm.autonotebook import tqdm
import numpy as np

duplicates = []


# Near-duplicates are identified by comparing cosine similarities between sentence embeddings; if similarity exceeds a threshold, items are considered duplicates.

def deduplicate_dataset(dataset: Dataset, model: str, threshold: float):
    sentence_model = SentenceTransformer(model)
    instruction = [example["instruction"] for example in dataset['train']]

    print("Converting text to embeddings...")
    embeddings = sentence_model.encode(instruction, show_progress_bar=True)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatIP(dimension)
    normalized_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
    index.add(normalized_embeddings)

    print("Filtering out near-duplicates...")
    D, I = index.search(normalized_embeddings, k=2)
    to_keep = []

    for i in tqdm(range(len(embeddings)), desc="Filtering"):
        # If the second closest vector (D[i, 1]) has cosine similarity above the threshold
        if D[i, 1] >= threshold:
            # Check if either the current item or its nearest neighbor is already in the to_keep list
            nearest_neighbor = I[i, 1]
            if i not in to_keep and nearest_neighbor not in to_keep:
                # If not, add the current item to the list
                to_keep.append(i)
            else:
                duplicates.append(i)
        else:
            # If the similarity is below the threshold, always keep the current item
            to_keep.append(i)

    dataset = dataset['train'].select(to_keep)
    return DatasetDict({"train": dataset})

deduped_dataset = deduplicate_dataset(dataset, "thenlper/gte-large", 0.95)

len(duplicates)

Duplicates_dataset = DatasetDict({"train": dataset['train'].select(duplicates)})
df = Duplicates_dataset['train'].to_pandas()

# Sort the DataFrame based on the 'text' column
# df = df.sort_values(by='instruction')

# Display the full text
for full_text in df['instruction'][:10]:
    print(full_text)

print(f"Number of samples in the original dataset: {len(dataset['train'])}")
print(f"Number of samples in the deduped dataset: {len(deduped_dataset['train'])}")
print(f"Number of samples that were removed: {len(dataset['train']) - len(deduped_dataset['train'])}")

"""We can clearly see that there aren't many duplications in the questions. So, we have decided to drop this idea."""

df = dataset['train'].to_pandas()
df

# I used the prompt template suggested by the officials. Find more details at https://gpus.llm-utils.org/llama-2-prompt-template/

llama2_prompt_without_input = """[INST] <> {instruction} <> [/INST] {output} """
llama2_prompt_with_input = """[INST] <> {instruction} <> {input} [/INST] {output} """

import itertools
examples = list(itertools.islice(dataset['train'], len(dataset['train'])))

print(examples[0]['instruction'])
print(examples[0]['input'])
print(examples[0]['output'])

transformed_dataset = []
for i in examples:
  if not i['input']:
    processed_prompt = llama2_prompt_without_input.format(instruction = i['instruction'], output = i['output'])
  else:
    processed_prompt = llama2_prompt_with_input.format(instruction = i['instruction'], input = i['input'], output = i['output'])

  transformed_dataset.append({'text': processed_prompt})

!pip install -q jsonlines
import jsonlines
with jsonlines.open(f'html_dataset.jsonl', 'w') as writer:
  writer.write_all(transformed_dataset)

filename = 'html_dataset.jsonl'

html_dataset_for_llama2_finetuning = load_dataset("json", data_files = filename)

html_dataset_for_llama2_finetuning

df = html_dataset_for_llama2_finetuning['train'].to_pandas()
df

from sklearn.model_selection import train_test_split

# Splitting into training (60%), testing (20%), and validation (20%) sets
train_data, temp_data = train_test_split(df, test_size=0.3)
test_data, validation_data = train_test_split(temp_data, test_size=0.5)

# Display the sizes of the resulting sets
print("Training set size:", len(train_data))
print("Testing set size:", len(test_data))
print("Validation set size:", len(validation_data))

train_data

import datasets

train_dataset = Dataset.from_dict(train_data[:2000])
test_dataset = Dataset.from_dict(test_data[:200])
validation_dataset = Dataset.from_dict(validation_data[:200])
final_data = datasets.DatasetDict({"train":train_dataset,"test":test_dataset, "validation": validation_dataset})

final_data

final_data.push_to_hub('Guna0pro/HTML-Data', token = hf_token)

